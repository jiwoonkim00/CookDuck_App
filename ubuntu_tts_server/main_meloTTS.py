import os
import time
import torch
from transformers import (
    AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline,
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
)
from melo.api import TTS

# ==================== GPU ì„¤ì • ====================
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "2"  # RTX 3070ì„ ì‚¬ìš©í•˜ë ¤ë©´ 2ë¡œ ì„¤ì •

# ==================== GPU ì‚¬ìš©ëŸ‰ ì²´í¬ í•¨ìˆ˜ ====================
def get_gpu_memory():
    if torch.cuda.is_available():
        gpu_id = torch.cuda.current_device()
        total = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**2)
        reserved = torch.cuda.memory_reserved(gpu_id) / (1024**2)
        allocated = torch.cuda.memory_allocated(gpu_id) / (1024**2)
        free = reserved - allocated
        name = torch.cuda.get_device_name(gpu_id)
        return {
            "GPU ID": gpu_id,
            "GPU Name": name,
            "Total MB": round(total, 2),
            "Reserved MB": round(reserved, 2),
            "Allocated MB": round(allocated, 2),
            "Free MB": round(free, 2),
        }
    return {"GPU": "not available"}

# ==================== LLM ì‘ë‹µ í•¨ìˆ˜ ì •ì˜ ====================
def ask_cooking_question(instruction, question_text, model, tokenizer):
    prompt = f"[INSTRUCTION]\n{instruction}\n[INPUT]\n{question_text}\n[OUTPUT]\n"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=False
    )
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.split("[OUTPUT]\n")[-1].strip()

# ==================== ì „ì²´ ì²˜ë¦¬ ì‹œì‘ ====================
total_start = time.perf_counter()

# ===== [1] STT =====
print("\nğŸ”Š STT ì‹œì‘")
stt_model_id = "openai/whisper-large-v3-turbo"
device = "cuda" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

stt_model = AutoModelForSpeechSeq2Seq.from_pretrained(
    stt_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
).to(device)
stt_processor = AutoProcessor.from_pretrained(stt_model_id)
stt_pipe = pipeline(
    "automatic-speech-recognition",
    model=stt_model,
    tokenizer=stt_processor.tokenizer,
    feature_extractor=stt_processor.feature_extractor,
    torch_dtype=torch_dtype,
    device=device,
)

# ì˜ì–´ ë²ˆì—­ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
stt_start = time.perf_counter() # textë„£ê¸° ì‹œì‘
stt_result = stt_pipe("./stt-llm-pipeline/audio/output_en_test.wav", generate_kwargs={"language": "korean"})
stt_text = stt_result["text"]
stt_end = time.perf_counter()
print(f"ğŸ“ STT ê²°ê³¼: {stt_text}")
print(f"â±ï¸ STT ì‹œê°„: {stt_end - stt_start:.2f}ì´ˆ")

# ===== [2] LLM =====
print("\nğŸ§  LLM ì‹œì‘")

llm_model_path = "torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)
llm_model = AutoModelForCausalLM.from_pretrained(
    llm_model_path,
    quantization_config=quant_config,
    torch_dtype=torch.float16,
    device_map="auto"
)
# instruction = "Give me an accurate answer about cooking without overlapping answers"
# ë ˆì‹œí”¼ í•™ìŠµ ë¯¸í¡ -> ì¼ë‹¨ ëŒ€ë‹µ ì˜ ë˜ëŠ” ë¶€ë¶„
llm_start = time.perf_counter() # ì§ˆë¬¸ ì‹œì‘
instruction = "Translate the following sentence into Korean naturally and accurately"
llm_answer = ask_cooking_question(instruction, stt_text, llm_model, llm_tokenizer)
llm_end = time.perf_counter()
print(f"ğŸ³ LLM ì‘ë‹µ: {llm_answer}")
print(f"â±ï¸ LLM ì‹œê°„: {llm_end - llm_start:.2f}ì´ˆ")

# ===== [3] TTS =====
print("\nğŸ—£ï¸ TTS ì‹œì‘")

# TTS ì„¤ì •
speed = 1.3
device = 'cuda:0'  # ë˜ëŠ” 'cuda:0'
text = llm_answer

# ì‹œê°„ ì¸¡ì • ì‹œì‘

# ëª¨ë¸ ë¡œë“œ ë° ìŒì„± í•©ì„±
model = TTS(language='KR', device=device)
speaker_ids = model.hps.data.spk2id
output_path = './result_audio/en_to_ko_test.wav'

tts_start = time.perf_counter() # tts ê²°ê³¼ ì‹œì‘
model.tts_to_file(text, speaker_ids['KR'], output_path, speed=speed)


tts_end = time.perf_counter()
print(f"ğŸ§ TTS ì €ì¥ ì™„ë£Œ: ./result_audio/llama-1b_test.wav")
print(f"â±ï¸ TTS ì‹œê°„: {tts_end - tts_start:.2f}ì´ˆ")

# ===== [4] ì „ì²´ ì†Œìš” ì‹œê°„ ë° GPU ì‚¬ìš©ëŸ‰ ì¶œë ¥ =====
total_end = time.perf_counter()
print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_end - total_start:.2f}ì´ˆ")
gpu_info = get_gpu_memory()
print("âœ… GPU ì‚¬ìš© ì •ë³´:")
for k, v in gpu_info.items():
    print(f"   {k}: {v}")
