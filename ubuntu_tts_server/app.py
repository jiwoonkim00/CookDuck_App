# # /work-space/cookDuck_llama/ubuntu_tts_server/app.py

# import uvicorn
# from fastapi import FastAPI, UploadFile, File, HTTPException
# from fastapi.responses import FileResponse
# from pydantic import BaseModel
# import shutil
# import os
# import logging
# import time
# import torch
# from transformers import (
#     AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline,
#     AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
# )
# from melo.api import TTS

# # ==================== GPU ì„¤ì • (ê°€ì¥ ì¤‘ìš”) ====================
# # nvidia-smi ìƒì—ì„œ 2ë²ˆ GPU(RTX 3070)ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
# # ì´ ì½”ë“œëŠ” torch ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë¡œë“œë˜ê¸° ì „ì— ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# os.environ["CUDA_VISIBLE_DEVICES"] = "2"

# # ==================== ë¡œê¹… ë° ê¸°ë³¸ ì„¤ì • ====================
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)
# # ì„ì‹œ íŒŒì¼ê³¼ ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
# os.makedirs("temp_files", exist_ok=True)
# os.makedirs("result_audio", exist_ok=True)

# app = FastAPI()

# # ==================== ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë° íŒŒì´í”„ë¼ì¸ ì €ì¥ ====================
# # ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ì´ ë³€ìˆ˜ë“¤ì— ëª¨ë¸ì´ ë¡œë“œë©ë‹ˆë‹¤.
# stt_pipe = None
# llm_model = None
# llm_tokenizer = None
# tts_model = None
# # ì´ì œ 'cuda'ëŠ” ìœ„ì—ì„œ ì§€ì •í•œ 2ë²ˆ GPUë¥¼ ê°€ë¦¬í‚¤ê²Œ ë©ë‹ˆë‹¤.
# device = "cuda" if torch.cuda.is_available() else "cpu"
# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# # ==================== LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ====================
# def ask_llm(instruction: str, question_text: str):
#     """LLM ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
#     global llm_model, llm_tokenizer
#     prompt = f"[INSTRUCTION]\n{instruction}\n[INPUT]\n{question_text}\n[OUTPUT]\n"
#     inputs = llm_tokenizer(prompt, return_tensors="pt").to(llm_model.device)
#     outputs = llm_model.generate(
#         **inputs,
#         max_new_tokens=256,
#         do_sample=True,
#         top_p=0.9,
#         temperature=0.7,
#     )
#     result = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return result.split("[OUTPUT]\n")[-1].strip()

# # ==================== ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”© (í•µì‹¬) ====================
# @app.on_event("startup")
# def load_models():
#     """ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ëª¨ë“  AI ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤."""
#     global stt_pipe, llm_model, llm_tokenizer, tts_model

#     logger.info("="*50)
#     logger.info("ì„œë²„ ì‹œì‘... AI ëª¨ë¸ì„ ì „ì—­ ë©”ëª¨ë¦¬ì— ë¡œë”©í•©ë‹ˆë‹¤.")
#     logger.info(f"ì‚¬ìš© ì¥ì¹˜: {device} (ì‹¤ì œ GPU: nvidia-smi 2ë²ˆ)", )
#     logger.info(f"ë°ì´í„° íƒ€ì…: {torch_dtype}")
#     logger.info("="*50)
    
#     # 1. STT ëª¨ë¸ ë¡œë”© (Whisper)
#     logger.info("ğŸ”Š STT ëª¨ë¸ ë¡œë”© ì‹œì‘...")
#     stt_model_id = "openai/whisper-large-v3"
#     stt_model_obj = AutoModelForSpeechSeq2Seq.from_pretrained(
#         stt_model_id, 
#         torch_dtype=torch_dtype, 
#         low_cpu_mem_usage=True, 
#         use_safetensors=True,
#         # ==================== [ìˆ˜ì •ëœ ë¶€ë¶„] ====================
#         # í˜¸í™˜ì„±ì„ ìœ„í•´ eager attention ì‚¬ìš©
#         attn_implementation="eager"
#         # =======================================================
#     ).to(device)
#     stt_processor = AutoProcessor.from_pretrained(stt_model_id)
#     stt_pipe = pipeline(
#         "automatic-speech-recognition",
#         model=stt_model_obj,
#         tokenizer=stt_processor.tokenizer,
#         feature_extractor=stt_processor.feature_extractor,
#         max_new_tokens=128,
#         chunk_length_s=30,
#         batch_size=16,
#         torch_dtype=torch_dtype,
#         device=device,
#     )
#     logger.info("âœ… STT ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

#     # 2. LLM ëª¨ë¸ ë¡œë”© (Llama-3.2 Korean GGACHI)
#     logger.info("ğŸ§  LLM ëª¨ë¸ ë¡œë”© ì‹œì‘...")
#     llm_model_path = "torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"
#     quant_config = BitsAndBytesConfig(
#         load_in_4bit=True,
#         bnb_4bit_quant_type="nf4",
#         bnb_4bit_compute_dtype=torch.float16
#     )
#     llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path)
#     llm_model = AutoModelForCausalLM.from_pretrained(
#         llm_model_path,
#         quantization_config=quant_config,
#         device_map="auto",
#         # ==================== [ìˆ˜ì •ëœ ë¶€ë¶„] ====================
#         # FlashAttention ëŒ€ì‹  ê°€ì¥ ì•ˆì •ì ì¸ 'eager' attentionì„ ì‚¬ìš©í•˜ë„ë¡ ëª…ì‹œ
#         attn_implementation="eager" 
#         # =======================================================
#     )
#     logger.info("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

#     # 3. TTS ëª¨ë¸ ë¡œë”© (MeloTTS)
#     logger.info("ğŸ—£ï¸ TTS ëª¨ë¸ ë¡œë”© ì‹œì‘...")
#     tts_model = TTS(language='KR', device=device)
#     logger.info("âœ… TTS ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    
#     logger.info("="*50)
#     logger.info("ğŸš€ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ. API ì„œë²„ê°€ ìš”ì²­ì„ ë°›ì„ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     logger.info("="*50)


# # ==================== API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ====================
# class LLMRequest(BaseModel):
#     """LLM/TTS ìš”ì²­ì„ ìœ„í•œ ë°ì´í„° ëª¨ë¸"""
#     # text: str
#     # instruction: str = "ë‹¹ì‹ ì€ ìš”ë¦¬ ë ˆì‹œí”¼ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” AI ìš”ë¦¬ì‚¬ 'ì¿¡ë•'ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ì¶° ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
#     text: str
#     instruction: str = "Translate the following sentence into Korean naturally and accurately."

# @app.post("/stt")
# async def speech_to_text_endpoint(audio: UploadFile = File(...)):
#     """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜"""
#     if not stt_pipe:
#         raise HTTPException(status_code=503, detail="STT ëª¨ë¸ì´ ì•„ì§ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

#     temp_input_path = f"temp_files/{audio.filename}"
#     try:
#         with open(temp_input_path, "wb") as buffer:
#             shutil.copyfileobj(audio.file, buffer)
        
#         logger.info(f"STT ì²˜ë¦¬ ì‹œì‘: {temp_input_path}")
#         stt_result = stt_pipe(temp_input_path, generate_kwargs={"language": "korean"})
#         recognized_text = stt_result["text"]
#         logger.info(f"STT ì²˜ë¦¬ ì™„ë£Œ: {recognized_text}")

#         return {"text": recognized_text}
#     except Exception as e:
#         logger.error(f"STT ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
#         raise HTTPException(status_code=500, detail=f"STT ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
#     finally:
#         if os.path.exists(temp_input_path):
#             os.remove(temp_input_path)

# @app.post("/generate-speech")
# async def generate_speech_endpoint(request: LLMRequest):
#     """í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ LLM ë‹µë³€ì„ ìƒì„±í•˜ê³  TTSë¡œ ë³€í™˜í•˜ì—¬ ìŒì„± íŒŒì¼ì„ ë°˜í™˜"""
#     if not llm_model or not tts_model:
#         raise HTTPException(status_code=503, detail="LLM ë˜ëŠ” TTS ëª¨ë¸ì´ ì•„ì§ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

#     try:
#         logger.info(f"LLM ì²˜ë¦¬ ì‹œì‘. Instruction: {request.instruction}, Text: {request.text}")
#         response_text = ask_llm(request.instruction, request.text)
#         logger.info(f"LLM ì²˜ë¦¬ ì™„ë£Œ: {response_text}")

#         logger.info(f"TTS ì²˜ë¦¬ ì‹œì‘...")
#         output_filename = f"response_{int(time.time())}.wav"
#         output_path = f"result_audio/{output_filename}"
        
#         speaker_ids = tts_model.hps.data.spk2id
#         tts_model.tts_to_file(response_text, speaker_ids['KR'], output_path, speed=1.1)
#         logger.info(f"TTS ì²˜ë¦¬ ì™„ë£Œ: {output_path}")

#         return FileResponse(path=output_path, media_type="audio/wav", filename=output_filename)
#     except Exception as e:
#         logger.error(f"LLM/TTS ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
#         raise HTTPException(status_code=500, detail=f"LLM/TTS ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)
# /work-space/cookDuck_llama/ubuntu_tts_server/app.py

# /work-space/cookDuck_llama/ubuntu_tts_server/app.py

# /work-space/cookDuck_llama/ubuntu_tts_server/app.py

import uvicorn
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
import shutil
import os
import logging
import time
import torch
from transformers import (
    AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline,
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
)
from melo.api import TTS

# --- GPU, ë¡œê¹…, ê¸°ë³¸ ì„¤ì • (ì´ì „ê³¼ ë™ì¼) ---
os.environ["CUDA_VISIBLE_DEVICES"] = "2"
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
os.makedirs("temp_files", exist_ok=True)
os.makedirs("result_audio", exist_ok=True)
app = FastAPI()

# --- ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
stt_pipe, llm_model, llm_tokenizer, tts_model = None, None, None, None
device = "cuda" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# --- LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def ask_llm(instruction: str, question_text: str):
    global llm_model, llm_tokenizer
    prompt = f"[INSTRUCTION]\n{instruction}\n[INPUT]\n{question_text}\n[OUTPUT]\n"
    inputs = llm_tokenizer(prompt, return_tensors="pt").to(llm_model.device)
    outputs = llm_model.generate(**inputs, max_new_tokens=256, do_sample=True, top_p=0.9, temperature=0.7)
    result = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.split("[OUTPUT]\n")[-1].strip()

# --- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”© (ì´ì „ê³¼ ë™ì¼) ---
@app.on_event("startup")
def load_models():
    # ... (ì´ì „ê³¼ ë™ì¼í•œ ëª¨ë¸ ë¡œë”© ë¡œì§)
    global stt_pipe, llm_model, llm_tokenizer, tts_model
    logger.info("="*50); logger.info("ì„œë²„ ì‹œì‘... AI ëª¨ë¸ì„ ì „ì—­ ë©”ëª¨ë¦¬ì— ë¡œë”©í•©ë‹ˆë‹¤."); logger.info(f"ì‚¬ìš© ì¥ì¹˜: {device} (ì‹¤ì œ GPU: nvidia-smi 2ë²ˆ)"); logger.info(f"ë°ì´í„° íƒ€ì…: {torch_dtype}"); logger.info("="*50)
    logger.info("ğŸ”Š STT ëª¨ë¸ ë¡œë”© ì‹œì‘..."); stt_model_id = "openai/whisper-large-v3"; stt_model_obj = AutoModelForSpeechSeq2Seq.from_pretrained(stt_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, attn_implementation="eager").to(device); stt_processor = AutoProcessor.from_pretrained(stt_model_id); stt_pipe = pipeline("automatic-speech-recognition", model=stt_model_obj, tokenizer=stt_processor.tokenizer, feature_extractor=stt_processor.feature_extractor, max_new_tokens=128, chunk_length_s=30, batch_size=16, torch_dtype=torch_dtype, device=device); logger.info("âœ… STT ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    logger.info("ğŸ§  LLM ëª¨ë¸ ë¡œë”© ì‹œì‘..."); llm_model_path = "torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"; quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.float16); llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_path); llm_model = AutoModelForCausalLM.from_pretrained(llm_model_path, quantization_config=quant_config, device_map="auto", attn_implementation="eager"); logger.info("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    logger.info("ğŸ—£ï¸ TTS ëª¨ë¸ ë¡œë”© ì‹œì‘..."); tts_model = TTS(language='KR', device=device); logger.info("âœ… TTS ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
    logger.info("="*50); logger.info("ğŸš€ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ. API ì„œë²„ê°€ ìš”ì²­ì„ ë°›ì„ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."); logger.info("="*50)


# ==================== API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ (ìˆ˜ì •ë¨) ====================
class LLMRequest(BaseModel):
    text: str
    instruction: str = "ë‹¹ì‹ ì€ ìš”ë¦¬ ë ˆì‹œí”¼ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” AI ìš”ë¦¬ì‚¬ 'ì¿¡ë•'ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ì¶° ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."

# STT ì—”ë“œí¬ì¸íŠ¸ (ì´ì „ê³¼ ë™ì¼)
@app.post("/stt")
async def speech_to_text_endpoint(audio: UploadFile = File(...)):
    # ... (ì´ì „ê³¼ ë™ì¼í•œ STT ë¡œì§)
    if not stt_pipe: raise HTTPException(status_code=503, detail="STT ëª¨ë¸ì´ ì•„ì§ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    temp_input_path = f"temp_files/{audio.filename}";  
    try:
        with open(temp_input_path, "wb") as buffer: shutil.copyfileobj(audio.file, buffer)
        logger.info(f"STT ì²˜ë¦¬ ì‹œì‘: {temp_input_path}"); stt_result = stt_pipe(temp_input_path, generate_kwargs={"language": "korean"}); recognized_text = stt_result["text"]; logger.info(f"STT ì²˜ë¦¬ ì™„ë£Œ: {recognized_text}")
        return {"text": recognized_text}
    except Exception as e: logger.error(f"STT ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}"); raise HTTPException(status_code=500, detail=f"STT ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        if os.path.exists(temp_input_path): os.remove(temp_input_path)


# LLM ì‘ë‹µ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@app.post("/generate-llm-response")
async def generate_llm_response_endpoint(request: LLMRequest):
    """LLM ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ìƒì„±ëœ í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì„ ë°˜í™˜"""
    if not llm_model or not tts_model:
        raise HTTPException(status_code=503, detail="LLM ë˜ëŠ” TTS ëª¨ë¸ì´ ì•„ì§ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        logger.info(f"LLM ì²˜ë¦¬ ì‹œì‘. Instruction: {request.instruction}, Text: {request.text}")
        response_text = ask_llm(request.instruction, request.text)
        logger.info(f"LLM ì²˜ë¦¬ ì™„ë£Œ: {response_text}")

        logger.info(f"TTS íŒŒì¼ ìƒì„± ì‹œì‘...")
        output_filename = f"response_{int(time.time())}.wav"
        output_path = f"result_audio/{output_filename}"
        
        speaker_ids = tts_model.hps.data.spk2id
        tts_model.tts_to_file(response_text, speaker_ids['KR'], output_path, speed=1.1)
        logger.info(f"TTS íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")

        return JSONResponse(content={"llm_text": response_text, "audio_filename": output_filename})
    except Exception as e:
        logger.error(f"LLM/TTS ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"LLM/TTS ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

# [ì‹ ê·œ] ì´ˆê¸° ì¸ì‚¬ë§ ìƒì„± ì—”ë“œí¬ì¸íŠ¸
@app.post("/generate-greeting")
async def generate_greeting_endpoint():
    """ì•± ì‹œì‘ ì‹œ ì´ˆê¸° ì¸ì‚¬ë§ ìŒì„±ì„ ìƒì„±í•˜ê³  í…ìŠ¤íŠ¸ì™€ íŒŒì¼ëª…ì„ ë°˜í™˜"""
    if not tts_model:
        raise HTTPException(status_code=503, detail="TTS ëª¨ë¸ì´ ì•„ì§ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        greeting_text = "ì•ˆë…•í•˜ì„¸ìš”! AI ìš”ë¦¬ì‚¬ ì¿¡ë•ì…ë‹ˆë‹¤. ì–´ë–¤ ìš”ë¦¬ì˜ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë“œë¦´ê¹Œìš”?"
        logger.info(f"ì´ˆê¸° ì¸ì‚¬ë§ ìƒì„±: {greeting_text}")

        output_filename = f"greeting_{int(time.time())}.wav"
        output_path = f"result_audio/{output_filename}"
        
        speaker_ids = tts_model.hps.data.spk2id
        tts_model.tts_to_file(greeting_text, speaker_ids['KR'], output_path, speed=1.1)
        logger.info(f"ì¸ì‚¬ë§ TTS íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")

        return JSONResponse(content={"llm_text": greeting_text, "audio_filename": output_filename})
    except Exception as e:
        logger.error(f"ì¸ì‚¬ë§ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail=f"ì¸ì‚¬ë§ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")


# ì˜¤ë””ì˜¤ íŒŒì¼ ì œê³µ ì—”ë“œí¬ì¸íŠ¸
@app.get("/audio/{filename}")
async def get_audio_file(filename: str):
    """`result_audio` ë””ë ‰í† ë¦¬ì—ì„œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì•„ ë°˜í™˜"""
    file_path = f"result_audio/{filename}"
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return FileResponse(path=file_path, media_type="audio/wav")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=False)


